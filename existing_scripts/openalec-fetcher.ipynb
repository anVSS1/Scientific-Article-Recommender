{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2538d96a-0e31-4ddf-af0e-5f6bf83a4a24",
    "_uuid": "6784199c-f981-4af0-8f5b-4ee17a14df3c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-05-27T19:56:09.464827Z",
     "iopub.status.busy": "2025-05-27T19:56:09.464156Z",
     "iopub.status.idle": "2025-05-27T19:56:28.684483Z",
     "shell.execute_reply": "2025-05-27T19:56:28.683838Z",
     "shell.execute_reply.started": "2025-05-27T19:56:09.464804Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OpenAlex Data Fetcher for Scientific Article Recommender\n",
    "\n",
    "This script fetches scientific articles and concept hierarchies from the OpenAlex API\n",
    "for use in the recommendation system. It focuses on Computer Science, AI, and Physics domains.\n",
    "\n",
    "Requirements:\n",
    "- requests library for API calls\n",
    "- tqdm for progress bars\n",
    "- Valid email for OpenAlex API (replace YOUR_EMAIL_HERE)\n",
    "\n",
    "Output:\n",
    "- openalex_works.jsonl: Scientific articles with metadata\n",
    "- openalex_concepts_hierarchy.jsonl: Concept hierarchies and relationships\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "EMAIL_FOR_OPENALEX = \"YOUR_EMAIL_HERE@example.com\"  # Replace with your actual email\n",
    "MAX_WORKS_TO_FETCH = 2000  # Maximum number of articles to fetch\n",
    "\n",
    "# Define research domains and their OpenAlex concept IDs\n",
    "# Each tuple contains (domain_name, concept_id, target_article_count)\n",
    "DOMAINS = [\n",
    "    (\"Computer Science\", \"https://openalex.org/C41008148\", 1400),\n",
    "    (\"Artificial Intelligence\", \"https://openalex.org/C154945302\", 400),\n",
    "    (\"Physics\", \"https://openalex.org/C121332964\", 200),\n",
    "]\n",
    "\n",
    "def reconstruct_abstract(index_dict):\n",
    "    \"\"\"\n",
    "    Reconstruct abstract text from OpenAlex inverted index format.\n",
    "    \n",
    "    OpenAlex stores abstracts as inverted indexes to save space:\n",
    "    {\"word\": [position1, position2], ...}\n",
    "    \n",
    "    Args:\n",
    "        index_dict (dict): Inverted index dictionary from OpenAlex\n",
    "        \n",
    "    Returns:\n",
    "        str: Reconstructed abstract text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the maximum position to determine text length\n",
    "        length = max(pos for positions in index_dict.values() for pos in positions) + 1\n",
    "        words = [''] * length\n",
    "        \n",
    "        # Place each word at its correct positions\n",
    "        for word, positions in index_dict.items():\n",
    "            for pos in positions:\n",
    "                words[pos] = word\n",
    "                \n",
    "        return ' '.join(words)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reconstructing abstract: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def fetch_openalex_data():\n",
    "    \"\"\"\n",
    "    Main function to fetch scientific articles and concepts from OpenAlex API.\n",
    "    \n",
    "    This function:\n",
    "    1. Iterates through defined research domains\n",
    "    2. Fetches articles with abstracts for each domain\n",
    "    3. Extracts concepts and topics from articles\n",
    "    4. Saves data to JSONL files for further processing\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    all_works = []\n",
    "    concepts = set()  # Use set to avoid duplicate concepts\n",
    "\n",
    "    # Process each research domain\n",
    "    for domain_name, concept_id, target_count in DOMAINS:\n",
    "        domain_works = []\n",
    "        page = 1\n",
    "        per_page = 100  # OpenAlex API limit\n",
    "        \n",
    "        print(f\"Fetching {target_count} works from {domain_name}...\")\n",
    "        \n",
    "        with tqdm(total=target_count, desc=f\"Fetching {domain_name}\") as pbar:\n",
    "            while len(domain_works) < target_count:\n",
    "                # API parameters for filtering and pagination\n",
    "                params = {\n",
    "                    \"filter\": f\"concepts.id:{concept_id},has_abstract:true\",  # Only articles with abstracts\n",
    "                    \"per_page\": per_page,\n",
    "                    \"page\": page,\n",
    "                    \"mailto\": EMAIL_FOR_OPENALEX  # Required for API access\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    # Make API request\n",
    "                    response = requests.get(base_url, params=params)\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    results = data.get(\"results\", [])\n",
    "                    if not results:\n",
    "                        print(f\"No more results for {domain_name}\")\n",
    "                        break\n",
    "\n",
    "                    # Process each article\n",
    "                    for work in results:\n",
    "                        # Reconstruct abstract from inverted index\n",
    "                        abstract_raw = work.get(\"abstract_inverted_index\", {})\n",
    "                        abstract = reconstruct_abstract(abstract_raw) if abstract_raw else \"\"\n",
    "\n",
    "                        # Extract article metadata\n",
    "                        article_data = {\n",
    "                            \"id\": work[\"id\"],\n",
    "                            \"title\": work[\"title\"],\n",
    "                            \"abstract\": abstract,\n",
    "                            \"publication_year\": work.get(\"publication_year\"),\n",
    "                            \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
    "                            \"authors\": [\n",
    "                                {\n",
    "                                    \"id\": a.get(\"author\", {}).get(\"id\"), \n",
    "                                    \"name\": a.get(\"author\", {}).get(\"display_name\")\n",
    "                                }\n",
    "                                for a in work.get(\"authorships\", [])\n",
    "                            ],\n",
    "                            \"institutions\": [\n",
    "                                {\"id\": i.get(\"id\"), \"name\": i.get(\"display_name\")}\n",
    "                                for a in work.get(\"authorships\", []) \n",
    "                                for i in a.get(\"institutions\", [])\n",
    "                            ],\n",
    "                            \"topics\": [\n",
    "                                {\n",
    "                                    \"id\": t[\"id\"], \n",
    "                                    \"name\": t[\"display_name\"], \n",
    "                                    \"level\": t.get(\"level\", 0), \n",
    "                                    \"wikidata\": t.get(\"wikidata\")\n",
    "                                }\n",
    "                                for t in work.get(\"topics\", [])\n",
    "                            ],\n",
    "                            \"concepts\": [\n",
    "                                {\n",
    "                                    \"id\": c[\"id\"], \n",
    "                                    \"name\": c[\"display_name\"], \n",
    "                                    \"level\": c.get(\"level\", 0), \n",
    "                                    \"wikidata\": c.get(\"wikidata\")\n",
    "                                }\n",
    "                                for c in work.get(\"concepts\", [])\n",
    "                            ],\n",
    "                            \"domain\": domain_name\n",
    "                        }\n",
    "                        \n",
    "                        domain_works.append(article_data)\n",
    "\n",
    "                        # Collect unique concepts for ontology\n",
    "                        for c in work.get(\"concepts\", []) + work.get(\"topics\", []):\n",
    "                            concept_data = {\n",
    "                                \"id\": c[\"id\"],\n",
    "                                \"name\": c[\"display_name\"],\n",
    "                                \"level\": c.get(\"level\", 0),\n",
    "                                \"wikidata\": c.get(\"wikidata\")\n",
    "                            }\n",
    "                            concepts.add(json.dumps(concept_data))\n",
    "\n",
    "                    pbar.update(len(results))\n",
    "                    page += 1\n",
    "\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"API Error in {domain_name} (page {page}): {e}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Unexpected error in {domain_name} (page {page}): {e}\")\n",
    "                    break\n",
    "\n",
    "        # Limit to target count for this domain\n",
    "        all_works.extend(domain_works[:target_count])\n",
    "\n",
    "    # Save articles to JSONL file\n",
    "    output_works_file = \"openalex_works.jsonl\"\n",
    "    with open(output_works_file, \"w\", encoding='utf-8') as f:\n",
    "        for work in all_works[:MAX_WORKS_TO_FETCH]:\n",
    "            f.write(json.dumps(work, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Save concepts to JSONL file\n",
    "    output_concepts_file = \"openalex_concepts_hierarchy.jsonl\"\n",
    "    with open(output_concepts_file, \"w\", encoding='utf-8') as f:\n",
    "        for concept_json in concepts:\n",
    "            f.write(concept_json + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… Data fetching completed!\")\n",
    "    print(f\"ðŸ“„ Saved {len(all_works)} articles to {output_works_file}\")\n",
    "    print(f\"ðŸ”— Saved {len(concepts)} unique concepts to {output_concepts_file}\")\n",
    "\n",
    "# Execute the data fetching\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_openalex_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T20:19:52.591880Z",
     "iopub.status.busy": "2025-05-27T20:19:52.591503Z",
     "iopub.status.idle": "2025-05-27T20:19:53.281068Z",
     "shell.execute_reply": "2025-05-27T20:19:53.280360Z",
     "shell.execute_reply.started": "2025-05-27T20:19:52.591859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preprocessing for Scientific Article Recommender\n",
    "\n",
    "This script processes the raw OpenAlex data fetched in the previous cell,\n",
    "cleaning and structuring it for use in the recommendation system.\n",
    "\n",
    "Input files:\n",
    "- openalex_works.jsonl: Raw article data from OpenAlex\n",
    "- openalex_concepts_hierarchy.jsonl: Raw concept data from OpenAlex\n",
    "\n",
    "Output files:\n",
    "- processed_articles.json: Clean article data for recommendation engine\n",
    "- processed_concepts.json: Clean concept data for ontology\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def preprocess_data(works_file=\"openalex_works.jsonl\", \n",
    "                   concepts_file=\"openalex_concepts_hierarchy.jsonl\",\n",
    "                   output_dir=\"../../data/cleaned data/\"):\n",
    "    \"\"\"\n",
    "    Preprocess raw OpenAlex data for the recommendation system.\n",
    "    \n",
    "    Args:\n",
    "        works_file (str): Path to raw works JSONL file\n",
    "        concepts_file (str): Path to raw concepts JSONL file\n",
    "        output_dir (str): Directory to save processed files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ”„ Starting data preprocessing...\")\n",
    "    \n",
    "    # Process articles/works\n",
    "    print(\"ðŸ“š Processing articles...\")\n",
    "    works = []\n",
    "    \n",
    "    try:\n",
    "        with open(works_file, \"r\", encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(tqdm(f, desc=\"Processing works\"), 1):\n",
    "                try:\n",
    "                    work = json.loads(line.strip())\n",
    "                    \n",
    "                    # Clean and structure article data\n",
    "                    processed_work = {\n",
    "                        \"id\": work[\"id\"],\n",
    "                        \"title\": work[\"title\"] or \"Untitled\",\n",
    "                        \"abstract\": work[\"abstract\"] or \"\",\n",
    "                        \"publication_year\": work[\"publication_year\"] or 0,\n",
    "                        \"cited_by_count\": work[\"cited_by_count\"] or 0,\n",
    "                        \"authors\": work.get(\"authors\", []),\n",
    "                        # Remove duplicate institutions while preserving order\n",
    "                        \"institutions\": list({\n",
    "                            i[\"id\"]: i for i in work.get(\"institutions\", []) \n",
    "                            if i.get(\"id\")\n",
    "                        }.values()),\n",
    "                        # Extract only top-level topics (level 0) for better classification\n",
    "                        \"topics\": [\n",
    "                            t[\"id\"] for t in work.get(\"topics\", []) \n",
    "                            if t.get(\"level\", 0) == 0\n",
    "                        ],\n",
    "                        # Include all concept IDs for broader coverage\n",
    "                        \"concepts\": [\n",
    "                            c[\"id\"] for c in work.get(\"concepts\", []) \n",
    "                            if c.get(\"id\")\n",
    "                        ],\n",
    "                        \"domain\": work.get(\"domain\", \"Unknown\")\n",
    "                    }\n",
    "                    \n",
    "                    works.append(processed_work)\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"âš ï¸  JSON error in works file at line {line_num}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Error processing work at line {line_num}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Works file not found: {works_file}\")\n",
    "        return\n",
    "    \n",
    "    # Process concepts\n",
    "    print(\"ðŸ”— Processing concepts...\")\n",
    "    concepts = {}\n",
    "    \n",
    "    try:\n",
    "        with open(concepts_file, \"r\", encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(tqdm(f, desc=\"Processing concepts\"), 1):\n",
    "                try:\n",
    "                    concept = json.loads(line.strip())\n",
    "                    \n",
    "                    # Create clean concept entry\n",
    "                    concept_id = concept[\"id\"]\n",
    "                    concepts[concept_id] = {\n",
    "                        \"id\": concept_id,\n",
    "                        \"name\": concept[\"name\"] or \"Unknown\",\n",
    "                        \"level\": concept.get(\"level\", 0),\n",
    "                        \"wikidata\": concept.get(\"wikidata\", \"\")\n",
    "                    }\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"âš ï¸  JSON error in concepts file at line {line_num}: {e}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Error processing concept at line {line_num}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Concepts file not found: {concepts_file}\")\n",
    "        return\n",
    "    \n",
    "    # Save processed data\n",
    "    print(\"ðŸ’¾ Saving processed data...\")\n",
    "    \n",
    "    try:\n",
    "        # Save articles\n",
    "        articles_output = os.path.join(output_dir, \"processed_articles.json\")\n",
    "        articles_df = pd.DataFrame(works)\n",
    "        articles_df.to_json(articles_output, orient=\"records\", indent=2, force_ascii=False)\n",
    "        \n",
    "        # Save concepts\n",
    "        concepts_output = os.path.join(output_dir, \"processed_concepts.json\")\n",
    "        concepts_df = pd.DataFrame.from_dict(concepts, orient=\"index\")\n",
    "        concepts_df.to_json(concepts_output, orient=\"records\", indent=2, force_ascii=False)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"âœ… Data preprocessing completed!\")\n",
    "        print(f\"ðŸ“Š Summary:\")\n",
    "        print(f\"   - Articles processed: {len(works)}\")\n",
    "        print(f\"   - Concepts processed: {len(concepts)}\")\n",
    "        print(f\"   - Articles saved to: {articles_output}\")\n",
    "        print(f\"   - Concepts saved to: {concepts_output}\")\n",
    "        \n",
    "        # Display domain distribution\n",
    "        if works:\n",
    "            domain_counts = pd.DataFrame(works)['domain'].value_counts()\n",
    "            print(f\"ðŸ“ˆ Domain distribution:\")\n",
    "            for domain, count in domain_counts.items():\n",
    "                print(f\"   - {domain}: {count} articles\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving processed data: {e}\")\n",
    "\n",
    "# Execute the preprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7521920,
     "sourceId": 11962287,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7531109,
     "sourceId": 11975599,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
